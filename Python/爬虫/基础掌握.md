## 涉及到的网络请求模块

* urllib模块

	> 该模块产生较早，使用繁琐，不建议使用

* requests模块

	> Python原生网络请求模块，功能强大、简单便捷、效率高效。



## requests获取数据

```python
pip install requests
```

```python
import requests
from tkinter import messagebox

def getData():
    url = 'https://cn.bing.com/search?'
    headers = {
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        'q':'搜索词条'
    }
    response = requests.get(url=url,params=param,headers=headers)
    page_text = response.text
    print(page_text)
    messagebox.showerror('提示','提示内容！！！')
```





## 数据解析

* 正则
* bs4
* xpath(<font color=ff00aa>重要</font>)



## 正则

[正则表达式必知必会](../../互联网技术/正则表达式必知必会.md)



## xpath

### xpath解析流程

1. 实例化一个etree的对象，将别解析的内容或者页面源码加载到该对象中
2. 调用etree对象中的xpath方法结合xpath**表达式**定位内容以及捕获内容



### xpath安装

```python
pip install lxml
```



### xpath表达式

类似于css/js选择器，通过xpath表达式快速的获取html文本中指定内容

* /  ：表示从根节点开始定位。表示的是一个层级。
* // ：表示多个层级。可以表示从任意位置开始定位

> 详细用法移步 ： [XPath & CSS元素定位----一篇搞定_hello-alien的博客-CSDN博客](https://blog.csdn.net/chenmozhe22/article/details/124789926)

#### 1、标签定位

```bash
/html/body/div
#表示定位html标签下body标签下的div
/html///div
#表示定位html标签下任意层级下的div标签
```



#### 2、属性定位

```bash
//div[@attrName='attrValue']
#表示定位div的attrName属性为attrValue
```



#### 3、索引定位

```bash
//div[@class='song']/p[3]
#表示定位class位song的div下第三个p标签
```



#### 4、取标签内容

```bash
/text()  
# 获取的是标签中直系的文本内容
//text()
# 获取的是标签非直系的所有文本内容
```



#### 5、取标签属性

```bash
/@attrName
# 获取某个标签下指定属性值
```



#### 6、案例

```python
import requests
from tkinter import messagebox
from lxml import etree
import os

def getData():
    url = 'https://cn.bing.com/search?'
    headers = {
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        'q':'搜索词条'
    }
    response = requests.get(url=url,params=param,headers=headers)
    page_text = response.text
    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(page_text)
    file.close()
    print('文件已生成')
    #xpath解析
    parser = etree.HTMLParser(encoding="utf-8")
    tree = etree.parse(os.path.join(os.getcwd(), 'crawler.html'), parser=parser)
    #获取 必应搜索引擎的所有搜索结果的标题
    text = tree.xpath('/html/body/div/main/ol/li[@class="b_algo"]//h2/a/text()')
    print(text)

if __name__ == '__main__':
    getData()
```

```python
# 爬取图片
import os
import requests
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
}

def get_page():
    url = 'https://www.pearvideo.com/'
    page_text = requests.get(url=url, headers=headers).text
    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(page_text)
    file.close()
    print('文件已生成')
    # xpath解析
    parser = etree.HTMLParser(encoding="utf-8")
    tree = etree.parse(os.path.join(os.getcwd(), 'crawler.html'), parser=parser)
    # 获取 梨视频首页四个图片
    img_urls = []
    div = tree.xpath('//*[@id="vervideoTlist"]/div/ul')
    for ul in div:
        urls = ul.xpath('./li/div/a/div/div/div/@style')
        for url in urls:
            img_urls.append(url.replace('background-image: url(', '').replace(');', ''))

    return img_urls



def download(img_urls):
    for img in img_urls:
        print(img)
        img_text = requests.get(url=img).content
        img_name = img.split('/')[len(img.split('/')) - 1]
        with open(os.path.join(os.getcwd(), img_name), 'wb') as fp:
            fp.write(img_text)


if __name__ == '__main__':
    img_urls = get_page()
    download(img_urls)
```



## 模拟登录以及跳过验证码

```pip
pip install ddddocr
```



```python
def verification_code():
    ocr = ddddocr.DdddOcr()
    with open('./code_img/4.jpg', 'rb') as fp:
        img_bytes = fp.read()
        res = ocr.classification(img_bytes)
        print(res)
```



## 基于cookie操作

```python
#获取gitee 自己首页信息
from lxml import etree
import os
import ddddocr
import requests

def login():
    loginUrl = 'https://gitee.com/login'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        "encrypt_key": 'password',
        "utf8": True,
        "authenticity_token": 'xxxxx',
        "redirect_to_url":'',
        "user[login]": 'xxxxxx',
        "encrypt_data[user[password]]": 'xxxxx',
        "user[remember_me]": '0'
    }
    #使用session登录 获取请求token信息
    session = requests.Session()
    response = session.post(url=loginUrl, params=param, headers=headers)
    page_text = response.text
    if response.status_code == 200:
        print('登录成功!!')

    #再次请求接口 session本身会携带token
    index_url = 'https://gitee.com/'
    index_context = session.get(url=index_url, headers=headers).text

    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(index_context)
    file.close()
    print('文件已生成')
```



## 异步操作 - 线程池

> 多个URL同时发起网络请求，会阻塞执行直至上个请求处理完毕。

```python
import time
from multiprocessing.dummy import Pool

def get_page(name):
    print('正在下载 : ', name)
    time.sleep(2)
    print('下载成功 : ', name)

if __name__ == '__main__':
    list = ['aa', 'bb', 'cc', 'dd']
    start_time = time.time()
    # 实例化线程池对象
    pool = Pool(4)
    pool.map(get_page, list)
    print(time.time() - start_time)
```



## 异步操作 - 协程

* event_loop : 事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行。
* coroutine : 协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用async关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象。
* task : 任务，它是对协程对象的进一步封装，包含了任务的各个状态。 
* future : 代表将来执行或还没有执行的任务，实际上和task没有本质区别。
* Async : 定义一个协程.
* await : 用来挂起阻塞方法的执行。



### 协程基本使用

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method():
    time.sleep(3)
    print('异步方法执行完毕！！')

if __name__ == '__main__':
    method_obj = async_method()
    # 2. 创建时间循环对象
    loop = asyncio.get_event_loop()
    # 3. 将协程对象注册到循环事件对象中
    loop.run_until_complete(method_obj)
```



### task

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method():
    time.sleep(3)
    print('异步方法执行完毕！！')

if __name__ == '__main__':
    method_obj = async_method()
    # 2. 创建时间循环对象
    loop = asyncio.get_event_loop()
    # 3. 创建task
    task = loop.create_task(method_obj)
    print(task)
    # 4. 事件循环对象执行任务
    loop.run_until_complete(task)
    print(task)

```



### future

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method():
    time.sleep(3)
    print('异步方法执行完毕！！')

if __name__ == '__main__':
    method_obj = async_method()
    # 2. 创建时间循环对象
    loop = asyncio.get_event_loop()
    # 3. 创建future
    future = asyncio.ensure_future(method_obj)
    print(future)
    # 4. 事件循环对象执行任务
    loop.run_until_complete(future)
    print(future)
```



### 执行回调

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method():
    time.sleep(2)
    print('异步方法执行完毕！！')
    return '五香豆腐干'

# 4. 定义回调函数
def callback_func(task):
    print('执行回调逻辑...')
    print('获取任务执行结果', task.result())

if __name__ == '__main__':
    method_obj = async_method()
    # 2. 创建时间循环对象
    loop = asyncio.get_event_loop()
    # 3. 创建future
    future = asyncio.ensure_future(method_obj)
    print(future)
    # 5. 将回调函数绑定到任务对象中
    future.add_done_callback(callback_func)
    # 6. 事件循环对象执行任务
    loop.run_until_complete(future)
    print(future)
```



### 实战 - sleep同步模块

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method(url):
    print('正在下载', url)
    time.sleep(2)
    print('下载完毕', url)

if __name__ == '__main__':
    start = time.time()
    urls = [
        'www.baidu.com',
        'www.google.com',
        'www.souhu.com'
    ]
    # 用来存放任务对象
    tasks = []
    for url in urls:
        method_obj = async_method(url)
        task = asyncio.ensure_future(method_obj)
        tasks.append(task)

    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    print(time.time()-start)
    
> 6.008452653884888
```



`问题:`

并没有想象中耗时2s，而是耗时6s，因为什么?

> 因为在异步协程中使用了同步模块相关的代码，无法实现异步，需要使用异步模块方法进行平替，例如上面的time.sleep(2)，则使用asyncio.sleep(2)平替，并且在使用异步模块方法进行平替的时候需要使用await进行修饰，这也很好理解不阻塞又一个异步。即最终使用await aysncio.sleep(2)

```python
import time
import asyncio

# 1. 定义异步协程方法
async def async_method(url):
    print('正在下载', url)
    await asyncio.sleep(2)
    print('下载完毕', url)

if __name__ == '__main__':
    start = time.time()
    urls = [
        'www.baidu.com',
        'www.google.com',
        'www.souhu.com'
    ]
    # 用来存放任务对象
    tasks = []
    for url in urls:
        method_obj = async_method(url)
        task = asyncio.ensure_future(method_obj)
        tasks.append(task)

    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    print(time.time()-start)
    
> 2.00075626373291
```



### 实战 - 同步requst模块

1、准备Node接口服务

```bash
npm init
npm install --save express
node xxx.js
```

```js
const express = require('express');
const app = express()

app.listen(5000, (port) => {
    console.log('listening on port ' + port);
})

app.get('/aa', (req, res) => {
    console.log('执行 [/aa] 接口')
    let {
        callback = Function.prototype
    } = req.query;

    let data = {
        code: 0,
        message: '[/aa]成功执行！'
    };
    setTimeout(() => {
        res.send(`${callback}(${JSON.stringify(data)})`)
    }, 2000);
})

app.get('/bb', (req, res) => {
    console.log('执行 [/bb] 接口')
    let {
        callback = Function.prototype
    } = req.query;

    let data = {
        code: 0,
        message: '[/bb]成功执行！'
    };
    setTimeout(() => {
        res.send(`${callback}(${JSON.stringify(data)})`)
    }, 2000);
})

app.get('/cc', (req, res) => {
    console.log('执行 [/cc] 接口')
    let {
        callback = Function.prototype
    } = req.query;

    let data = {
        code: 0,
        message: '[/cc]成功执行！'
    };
    setTimeout(() => {
        res.send(`${callback}(${JSON.stringify(data)})`)
    }, 2000);
})
```



```python
import time
import asyncio
import requests
import re

async def async_method(url):
    res = requests.get(url=url)
    msg = None
    if res.status_code == 200:
        text = res.text
        # 'function () { [native code] }({"code":0,"message":"[/aa]成功执行！"})'
        msg = re.match('.*message":"(.*)\"\}\)', text).group(1)
    else:
        msg = res.text

    print('执行完毕请求,请求结果为 ', msg)
    return msg


if __name__ == '__main__':
    start = time.time()
    urls = [
        'http://localhost:5000/aa',
        'http://localhost:5000/bb',
        'http://localhost:5000/cc'
    ]
    # 用来存放任务对象
    tasks = []
    for url in urls:
        method_obj = async_method(url)
        task = asyncio.ensure_future(method_obj)
        tasks.append(task)

    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    print(time.time() - start)
    
    
> 执行完毕请求,请求结果为  [/aa]成功执行！
> 执行完毕请求,请求结果为  [/bb]成功执行！
> 执行完毕请求,请求结果为  [/cc]成功执行！
> 6.0284178256988525    
```

> 同样的6s ，同样的原因和time.sleep。如何解决，使用异步http模块 **aiohttp**



2、aiohttp

```bash
npm install aiohttp
```

```python
import time
import asyncio
import requests
import re
import aiohttp

async def async_method(url):
    async with aiohttp.ClientSession() as session:
        async with await session.get(url) as response:
            #text()返回字筑串形式的响应数据
            #read()返回的二进制形式的响应数据
            #json()返回jsond对象
            #注意：获取响应数据操作之煎一定要使用wait进行手动挂起
            page_text = await response.text()
            msg = re.match('.*message":"(.*)\"\}\)', page_text).group(1)
            print('执行完毕请求,请求结果为 ', msg)
            return msg


if __name__ == '__main__':
    start = time.time()
    urls = [
        'http://localhost:5000/aa',
        'http://localhost:5000/bb',
        'http://localhost:5000/cc'
    ]
    # 用来存放任务对象
    tasks = []
    for url in urls:
        method_obj = async_method(url)
        task = asyncio.ensure_future(method_obj)
        tasks.append(task)

    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    print(time.time() - start)
```

