## 涉及到的网络请求模块

* urllib模块

	> 该模块产生较早，使用繁琐，不建议使用

* requests模块

	> Python原生网络请求模块，功能强大、简单便捷、效率高效。



## requests获取数据

```python
pip install requests
```

```python
import requests
from tkinter import messagebox

def getData():
    url = 'https://cn.bing.com/search?'
    headers = {
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        'q':'搜索词条'
    }
    response = requests.get(url=url,params=param,headers=headers)
    page_text = response.text
    print(page_text)
    messagebox.showerror('提示','提示内容！！！')
```





## 数据解析

* 正则
* bs4
* xpath(<font color=ff00aa>重要</font>)



## 正则

[正则表达式必知必会](../../互联网技术/正则表达式必知必会.md)



## xpath

### xpath解析流程

1. 实例化一个etree的对象，将别解析的内容或者页面源码加载到该对象中
2. 调用etree对象中的xpath方法结合xpath**表达式**定位内容以及捕获内容



### xpath安装

```python
pip install lxml
```



### xpath表达式

类似于css/js选择器，通过xpath表达式快速的获取html文本中指定内容

* /  ：表示从根节点开始定位。表示的是一个层级。
* // ：表示多个层级。可以表示从任意位置开始定位

#### 1、标签定位

```bash
/html/body/div
#表示定位html标签下body标签下的div
/html///div
#表示定位html标签下任意层级下的div标签
```



#### 2、属性定位

```bash
//div[@attrName='attrValue']
#表示定位div的attrName属性为attrValue
```



#### 3、索引定位

```bash
//div[@class='song']/p[3]
#表示定位class位song的div下第三个p标签
```



#### 4、取标签内容

```bash
/text()  
# 获取的是标签中直系的文本内容
//text()
# 获取的是标签非直系的所有文本内容
```



#### 5、取标签属性

```bash
/@attrName
# 获取某个标签下指定属性值
```



#### 6、案例

```python
import requests
from tkinter import messagebox
from lxml import etree
import os

def getData():
    url = 'https://cn.bing.com/search?'
    headers = {
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        'q':'搜索词条'
    }
    response = requests.get(url=url,params=param,headers=headers)
    page_text = response.text
    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(page_text)
    file.close()
    print('文件已生成')
    #xpath解析
    parser = etree.HTMLParser(encoding="utf-8")
    tree = etree.parse(os.path.join(os.getcwd(), 'crawler.html'), parser=parser)
    #获取 必应搜索引擎的所有搜索结果的标题
    text = tree.xpath('/html/body/div/main/ol/li[@class="b_algo"]//h2/a/text()')
    print(text)

if __name__ == '__main__':
    getData()
```

```python
# 爬取图片
import os
import requests
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
}

def get_page():
    url = 'https://www.pearvideo.com/'
    page_text = requests.get(url=url, headers=headers).text
    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(page_text)
    file.close()
    print('文件已生成')
    # xpath解析
    parser = etree.HTMLParser(encoding="utf-8")
    tree = etree.parse(os.path.join(os.getcwd(), 'crawler.html'), parser=parser)
    # 获取 梨视频首页四个图片
    img_urls = []
    div = tree.xpath('//*[@id="vervideoTlist"]/div/ul')
    for ul in div:
        urls = ul.xpath('./li/div/a/div/div/div/@style')
        for url in urls:
            img_urls.append(url.replace('background-image: url(', '').replace(');', ''))

    return img_urls



def download(img_urls):
    for img in img_urls:
        print(img)
        img_text = requests.get(url=img).content
        img_name = img.split('/')[len(img.split('/')) - 1]
        with open(os.path.join(os.getcwd(), img_name), 'wb') as fp:
            fp.write(img_text)


if __name__ == '__main__':
    img_urls = get_page()
    download(img_urls)
```



## 模拟登录以及跳过验证码

```pip
pip install ddddocr
```



```python
def verification_code():
    ocr = ddddocr.DdddOcr()
    with open('./code_img/4.jpg', 'rb') as fp:
        img_bytes = fp.read()
        res = ocr.classification(img_bytes)
        print(res)
```



## 基于cookie操作

```python
#获取gitee 自己首页信息
from lxml import etree
import os
import ddddocr
import requests

def login():
    loginUrl = 'https://gitee.com/login'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32'
    }
    param = {
        "encrypt_key": 'password',
        "utf8": True,
        "authenticity_token": 'xxxxx',
        "redirect_to_url":'',
        "user[login]": 'xxxxxx',
        "encrypt_data[user[password]]": 'xxxxx',
        "user[remember_me]": '0'
    }
    #使用session登录 获取请求token信息
    session = requests.Session()
    response = session.post(url=loginUrl, params=param, headers=headers)
    page_text = response.text
    if response.status_code == 200:
        print('登录成功!!')

    #再次请求接口 session本身会携带token
    index_url = 'https://gitee.com/'
    index_context = session.get(url=index_url, headers=headers).text

    # 文件存储
    file = open(os.path.join(os.getcwd(), 'crawler.html'), 'w')
    file.write(index_context)
    file.close()
    print('文件已生成')
```



## 异步操作 - 线程池

> 多个URL同时发起网络请求，会阻塞执行直至上个请求处理完毕。

```python
import time
from multiprocessing.dummy import Pool

def get_page(name):
    print('正在下载 : ', name)
    time.sleep(2)
    print('下载成功 : ', name)

if __name__ == '__main__':
    list = ['aa', 'bb', 'cc', 'dd']
    start_time = time.time()
    # 实例化线程池对象
    pool = Pool(4)
    pool.map(get_page, list)
    print(time.time() - start_time)
```





















